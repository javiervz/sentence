{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detection of important sentences over \"time\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classification by macroarea :)\n",
    "## https://glottolog.org/meta/downloads\n",
    "\n",
    "macroarea = pd.read_csv('languages_and_dialects_geo.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macroarea.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macroarea = macroarea[['isocodes','macroarea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dictionary iso_code:macroarea\n",
    "\n",
    "macroarea = dict(zip(macroarea['isocodes'], macroarea['macroarea']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter by languages of the americas\n",
    "\n",
    "macroarea = {language:macroarea[language] for language in macroarea.keys() if macroarea[language] in ['South America','North America']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#macroarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#with zipfile.ZipFile('udhr.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## corpus UDHR https://www.unicode.org/udhr/index.html\n",
    "\n",
    "languages={}\n",
    "\n",
    "for language in macroarea.keys():\n",
    "    #with open('udhr_' + language + '.txt', 'r', encoding='utf-8') as file:\n",
    "    try: \n",
    "        file = open('udhr/'+'udhr_'+language+'.txt', 'r')  \n",
    "        language_text=file.read().split('\\n')\n",
    "        languages[language]=[line.strip() for line in language_text]\n",
    "        languages[language]=[line for line in languages[language] if len(line)>0]\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapudungun :)\n",
    "\n",
    "languages['zro'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic statistics: types and tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return s.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(L):\n",
    "    language=languages[L]\n",
    "    table = str.maketrans({key: None for key in '``!\"#$%&\\¿()*+,-./:;<=>?@[\\\\]_{|}'})\n",
    "    language=[list(filter(None, [w.lower().translate(table) for w in tokenize(sentence)])) for sentence in language if len([w.lower() for w in [w.translate(table) for w in tokenize(sentence)]])>0]\n",
    "    language=[[w for w in s if w!=\"''\"] for s in language] \n",
    "    language=[[w for w in s if w!='̃'] for s in language] \n",
    "    language=[[w for w in s if not w.isdigit()] for s in language]\n",
    "    return language[9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_languages={}\n",
    "for language in languages:\n",
    "    C=clean(language)\n",
    "    if len(C)>0:\n",
    "        clean_languages[language]=clean(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_languages['kwi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for language in clean_languages.keys():\n",
    "#    print(language, clean_languages[language][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple statistics: types, tokens and entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_entropy={}\n",
    "words={}\n",
    "mean_tokens={}\n",
    "mean_types={}\n",
    "number_sentences={}\n",
    "for language in clean_languages.keys():\n",
    "    number_sentences[language]=len(clean_languages[language])\n",
    "    words_entropy[language]=[item for sublist in clean_languages[language] for item in sublist]\n",
    "    words[language]=[len([item for sublist in clean_languages[language] for item in sublist]),len(set([item for sublist in clean_languages[language] for item in sublist])),Counter([item for sublist in clean_languages[language] for item in sublist])]\n",
    "    mean_tokens[language]=len([item for sublist in clean_languages[language] for item in sublist])\n",
    "    mean_types[language]=len(set([item for sublist in clean_languages[language] for item in sublist]))\n",
    "    print(language,len(clean_languages[language]),len([item for sublist in clean_languages[language] for item in sublist]),len(set([item for sublist in clean_languages[language] for item in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=800)\n",
    "\n",
    "H=list(mean_tokens.values())\n",
    "data = np.array(H)\n",
    "plt.hist(data, bins='sturges', cumulative=False, linewidth=0.75, color='gold',alpha=0.95,histtype='stepfilled',stacked=False,density=False,\n",
    "        zorder=5, edgecolor='k')\n",
    "#H=list(mean_types.values())\n",
    "#data = np.array(H)\n",
    "#plt.hist(data, bins='doane', cumulative=False, color='r',alpha=0.75,stacked=False,density=True,\n",
    "#        zorder=5, edgecolor='k')\n",
    "#kde = sm.nonparametric.KDEUnivariate(data)\n",
    "#kde.fit(bw=0.8) # Estimate the densities\n",
    "#X=np.linspace(500, 3000, num=1000)\n",
    "\n",
    "#ax.plot(X, [kde.evaluate(x) for x in X], '-', lw=2, color='r', zorder=10)\n",
    "\n",
    "#plt.legend(loc='upper left',fontsize=12)\n",
    "#plt.xlim([0., 2])\n",
    "#plt.ylim([0., 3])\n",
    "print(np.mean(data))\n",
    "#plt.axvline(x=np.mean(data),linestyle='--',color='k')\n",
    "plt.xlabel(r'tokens',fontsize=15)\n",
    "plt.ylabel(r'frequency',fontsize=15)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.savefig('tokens.pdf', format='pdf', transparent=True, bbox_inches='tight',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=800)\n",
    "\n",
    "H=list(number_sentences.values())\n",
    "data = np.array(H)\n",
    "plt.hist(data, bins='sturges', cumulative=False, linewidth=0.75, color='lime',alpha=0.95,histtype='stepfilled',stacked=False,density=False,\n",
    "        zorder=5, edgecolor='k')\n",
    "#H=list(mean_types.values())\n",
    "#data = np.array(H)\n",
    "#plt.hist(data, bins='doane', cumulative=False, color='r',alpha=0.75,stacked=False,density=True,\n",
    "#        zorder=5, edgecolor='k')\n",
    "#kde = sm.nonparametric.KDEUnivariate(data)\n",
    "#kde.fit(bw=0.8) # Estimate the densities\n",
    "#X=np.linspace(500, 3000, num=1000)\n",
    "\n",
    "#ax.plot(X, [kde.evaluate(x) for x in X], '-', lw=2, color='r', zorder=10)\n",
    "\n",
    "#plt.legend(loc='upper left',fontsize=12)\n",
    "#plt.xlim([0., 2])\n",
    "#plt.ylim([0., 3])\n",
    "print(np.mean(data))\n",
    "#plt.axvline(x=np.mean(data),linestyle='--',color='k')\n",
    "plt.xlabel(r'number of sentences',fontsize=15)\n",
    "plt.ylabel(r'frequency',fontsize=15)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.savefig('sentences.pdf', format='pdf', transparent=True, bbox_inches='tight',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_languages['ayr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graphs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adjacency graph\n",
    "## n_sent: graph with n_sent sentences\n",
    "\n",
    "def GoW(text_clean,n_parts,radius=1):\n",
    "    list_graphs=[]\n",
    "    L = range(len(text_clean))\n",
    "    parts = np.array_split(L, n_parts)\n",
    "    \n",
    "    for part in parts:\n",
    "        G=nx.Graph()\n",
    "        for sentence in [text_clean[i] for i in part]:\n",
    "            if len(sentence)>1:\n",
    "                pairs=[]\n",
    "                for r in list(range(1,radius+1)):\n",
    "                    pairs+=list(zip(sentence,sentence[r:]))#+list(zip(sentence,sentence[2:]))+list(zip(sentence,sentence[3:]))\n",
    "                for pair in pairs:\n",
    "                    if G.has_edge(pair[0],pair[1])==False:\n",
    "                        G.add_edge(pair[0],pair[1],weight=1)\n",
    "                    else:\n",
    "                        x=G[pair[0]][pair[1]]['weight']\n",
    "                        G[pair[0]][pair[1]]['weight']=x+1\n",
    "                    \n",
    "        #Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "        #G0 = G.subgraph(Gcc[0])\n",
    "        list_graphs+=[G]\n",
    "    \n",
    "    return list_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs={L:{} for L in clean_languages.keys()}\n",
    "\n",
    "\n",
    "for language in clean_languages.keys():\n",
    "    if mean_tokens[language]>mean_types[language]:\n",
    "        #print(language)\n",
    "        for n_parts in [1,2,3,4,5,6,7,8,9,10]:\n",
    "            G=GoW(clean_languages[language],n_parts)\n",
    "            graphs[language][n_parts]=G#nx.maximum_spanning_tree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs['arn'][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### algoritmos!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity_dict = {language:{} for language in graphs.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in graphs.keys():\n",
    "    for n_part in graphs[language].keys():\n",
    "        list_graphs=graphs[language][n_part]\n",
    "        list_mod = {}\n",
    "        for i in range(len(list_graphs)):\n",
    "            G=list_graphs[i]\n",
    "            if len(G.edges())>0:\n",
    "                partition = community_louvain.best_partition(G)\n",
    "                list_mod[i+1]=community_louvain.modularity(partition,G)\n",
    "        modularity_dict[language][n_part]=list_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict = {language:{} for language in graphs.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in graphs.keys():\n",
    "    for n_part in graphs[language].keys():\n",
    "        list_graphs=graphs[language][n_part]\n",
    "        list_core = {}\n",
    "        for i in range(len(list_graphs)):\n",
    "            G=list_graphs[i]\n",
    "            G.remove_edges_from(nx.selfloop_edges(G))\n",
    "            list_core[i+1]=np.mean(list(nx.core_number(G).values()))\n",
    "        core_dict[language][n_part]=list_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_core_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in graphs.keys():\n",
    "    G=graphs[language][1][0]\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    k_core_dict[language]=len(nx.k_core(G))/len(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=800)\n",
    "    \n",
    "ax.plot([mean_types[L]/mean_tokens[L] for L in k_core_size_dict.keys()],[core_dict[L][1][1] for L in k_core_size_dict.keys()],'H',color='orange',markersize=6,markeredgewidth=0.5,markeredgecolor='k',alpha=0.75,fillstyle='full',clip_on=True)\n",
    "\n",
    "plt.grid(False)\n",
    "#plt.legend(loc='best')\n",
    "plt.ylabel(r'average core number',fontsize=12)\n",
    "plt.xlabel(r'type-token ratio',fontsize=12)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.savefig('core.pdf', format='pdf', transparent=True, bbox_inches='tight',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=800)\n",
    "    \n",
    "ax.plot([mean_types[L]/mean_tokens[L] for L in k_core_size_dict.keys()],[k_core_dict[L] for L in k_core_size_dict.keys()],'D',color='lime',markersize=6,markeredgewidth=0.5,markeredgecolor='k',alpha=0.75,fillstyle='full',clip_on=True)\n",
    "\n",
    "plt.grid(False)\n",
    "#plt.legend(loc='best')\n",
    "plt.ylabel(r'$|k-core|/|G|$',fontsize=9)\n",
    "plt.xlabel(r'type-token ratio',fontsize=9)\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "plt.savefig('corevsG.pdf', format='pdf', transparent=True, bbox_inches='tight',dpi=800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
